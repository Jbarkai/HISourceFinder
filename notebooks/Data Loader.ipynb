{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "import astropy.constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "import astropy.units as u\n",
    "from astropy.wcs import WCS\n",
    "from astropy.visualization import ImageNormalize, ZScaleInterval\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# example of using a pre-trained model as a classifier\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "import tensorflow as tf\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchvision import models\n",
    "from matplotlib.pyplot import cm\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# Apply the transformations needed\n",
    "import torchvision.transforms as T\n",
    "import seaborn as sns\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from medzoo_imports import create_model, DiceLoss, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from data_generators.data_loader import SegmentationDataSet\n",
    "import argparse\n",
    "from os import listdir\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "dataset_name=\"hi_data\"\n",
    "root = '../'\n",
    "dim=(64, 64, 64)\n",
    "nEpochs=10\n",
    "classes=2\n",
    "inChannels=1\n",
    "terminal_show_freq=50\n",
    "split=0.\n",
    "model='VNET'\n",
    "opt='sgd'\n",
    "log_dir='../runs/'\n",
    "lr = 1e-2\n",
    "params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 2}\n",
    "\n",
    "save = ('../saved_models/' + model + '_checkpoints/' + model + '_', dataset_name)[0]\n",
    "# input and target files\n",
    "inputs = [root+'Input/' + x for x in listdir(root+'Input') if \".fits\" in x]\n",
    "targets = [root+'Target/' + x for x in listdir(root+'Target') if \".fits\" in x]\n",
    "random_seed = 42\n",
    "train_size = 0.8\n",
    "dims = [128, 128, 64]\n",
    "overlaps = np.array(dims)-np.array([100, 100, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_valid = train_test_split(\n",
    "    inputs,\n",
    "    random_state=random_seed,\n",
    "    train_size=train_size,\n",
    "    shuffle=True)\n",
    "\n",
    "targets_train, targets_valid = train_test_split(\n",
    "    targets,\n",
    "    random_state=random_seed,\n",
    "    train_size=train_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset training\n",
    "dataset_train = SegmentationDataSet(inputs=inputs_train,\n",
    "                                    targets=targets_train,\n",
    "                                    dims=dims,\n",
    "                                    overlaps=overlaps,\n",
    "                                    load=False,\n",
    "                                    root=root)\n",
    "\n",
    "# dataset validation\n",
    "dataset_valid = SegmentationDataSet(inputs=inputs_valid,\n",
    "                                    targets=targets_valid,\n",
    "                                    dims=dims,\n",
    "                                    overlaps=overlaps,\n",
    "                                    load=False,\n",
    "                                    root=root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader training\n",
    "params = {'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2}\n",
    "dataloader_training = DataLoader(dataset=dataset_train, **params)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model . . . . . . . .VNET\n",
      "VNET Number of params: 45599926\n"
     ]
    }
   ],
   "source": [
    "class argsclass:\n",
    "    def __init__(self, model, opt, lr, inChannels, classes):\n",
    "        self.model = model\n",
    "        self.opt=opt\n",
    "        self.lr=lr\n",
    "        self.inChannels=1\n",
    "        self.classes=1\n",
    "        self.log_dir = \"./runs/\"\n",
    "        self.dataset_name = 'iseg2017'\n",
    "        self.save = save\n",
    "        self.terminal_show_freq = 50\n",
    "        self.nEpochs = 10\n",
    "        self.inModalities = 1\n",
    "        self.cuda = False\n",
    "args = argsclass('VNET', opt, lr, inChannels, classes)\n",
    "model, optimizer = create_model(args)\n",
    "criterion = DiceLoss(classes=args.classes)\n",
    "if os.path.exists(save):\n",
    "    shutil.rmtree(save)\n",
    "    os.mkdir(save)\n",
    "else:\n",
    "    os.makedirs(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args, model, criterion, optimizer, train_data_loader=dataloader_training,\n",
    "                        valid_data_loader=dataloader_validation, lr_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_tuple in trainer.train_data_loader:\n",
    "    input_tensor, target = input_tuple\n",
    "    output = trainer.model(input_tensor)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING...\n"
     ]
    }
   ],
   "source": [
    "print(\"START TRAINING...\")\n",
    "trainer.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cube_hdulist = fits.open(\"../data/training/Input/noisefree_1245mosC.fits\")\n",
    "cube_data = cube_hdulist[0].data\n",
    "cube_hdulist.close()\n",
    "\n",
    "maskcube_hdulist = fits.open(\"../data/training/Target/mask_1245mosC.fits\")\n",
    "maskcube_data = maskcube_hdulist[0].data\n",
    "maskcube_hdulist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cube_data[300, :, :])\n",
    "plt.show()\n",
    "plt.imshow(maskcube_data[300, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement sliding window\n",
    "From the closest distance (max frequency), figure out largest width of a galaxy cube = (42, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data_generators.cube_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_d = 50*u.Mpc\n",
    "h_0 = 70*u.km/(u.Mpc*u.s)\n",
    "noise_res = [15*u.arcsec, 25*u.arcsec]\n",
    "# Load Galaxy\n",
    "orig_mass, dx, dy, dF, rest_freq, orig_scale, gal_data = load_cube('../data/mock_gals/model1000s.fits', orig_d, h_0)\n",
    "# Choose max channel\n",
    "chosen_f = (rest_freq/(1+orig_d*h_0/const.c)).to(u.Hz)\n",
    "new_z = (rest_freq/chosen_f)-1\n",
    "new_dist = (const.c*new_z/h_0).to(u.Mpc)\n",
    "# chosen_f = , new_z, new_dist, z_pos = choose_freq(\n",
    "#     noise_spectral, noise_data.shape, gal_data.shape, rest_freq, h_0, orig_d)\n",
    "# Smooth cube\n",
    "smoothed_gal = smooth_cube(noise_res, new_z, new_dist, dx, dy, gal_data, orig_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdulist = fits.open(\"../data/mosaics/1245mosC.derip.fits\")\n",
    "noise_header = hdulist[0].header\n",
    "hdulist.close()\n",
    "# Regrid Cube\n",
    "resampled, new_dF = regrid_cube(smoothed_gal, noise_header, new_dist, dx, dy, dF, orig_scale, chosen_f, rest_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Minimum overlap width = \", resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_bin = (resampled > np.mean(resampled) + np.std(resampled)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range (44, 86):\n",
    "    plt.imshow(masked_bin[i, 150:250, 150:250])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "86-44, 100, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need 128 × 128 × 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [10, 500, 500]\n",
    "overlaps = [8, 400, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel=(1, dims[0], dims[1], dims[2], 1)\n",
    "stride=(1, overlaps[0], overlaps[1], overlaps[2], 1)    \n",
    "patches=tf.extract_volume_patches(\n",
    "    cube_data[280:300][None, ..., None],kernel,stride,'SAME',\n",
    ")\n",
    "_,x,y,z,n = patches.shape\n",
    "_,sx,sy,sz,_ = kernel\n",
    "patches = tf.reshape(patches,[x*y*z,sx,sy,sz])\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(patches)):\n",
    "    plt.imshow(patches[i][0, :, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
